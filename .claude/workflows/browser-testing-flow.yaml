name: Browser Testing Flow
description: Comprehensive browser-based UI testing and optimization workflow using Chrome DevTools MCP
type: browser-testing
project_type: ui-testing

# Error Recovery Configuration
recovery:
  enabled: true
  checkpoint_dir: '.claude/context/runs/{{run_id}}/checkpoints'

retry_config:
  max_attempts: 3
  backoff_strategy: exponential
  initial_delay_ms: 1000
  retryable_errors: [timeout, network_error, rate_limit]

fallback_agents:
  developer: [code-reviewer, qa]
  architect: [developer, security-architect]
  qa: [developer, code-reviewer]
  planner: [architect, pm]
  ux-expert: [pm, accessibility-expert]
  accessibility-expert: [ux-expert, qa]
  performance-engineer: [developer, architect]
  code-reviewer: [developer, ux-expert]

# NOTE: trigger_keywords here are for documentation only.
# The actual workflow selection uses keywords from .claude/config.yaml workflow_selection section.
# See .claude/config.yaml for the authoritative keyword list used by the routing system.
trigger_keywords:
  - 'browser test'
  - 'chrome devtools'
  - 'test all UI features'
  - 'browser automation'
  - 'test page'
  - 'find bugs browser'
  - 'test UI and optimize'
  - 'browser performance'

# Workflow inputs
inputs:
  target_url:
    type: string
    description: 'URL of the application to test (e.g., http://localhost:3000)'
    required: true
  documentation_sources:
    type: array
    description: 'Array of documentation paths to compare against (PRD, UX spec, user stories)'
    required: false
    default: []
  test_scope:
    type: string
    description: 'Scope of testing (full, critical_path, specific_features)'
    required: false
    default: 'full'
  performance_thresholds:
    type: object
    description: 'Performance thresholds for Core Web Vitals'
    required: false
    default:
      lcp_ms: 2500
      fid_ms: 100
      cls: 0.1
      fcp_ms: 1800
      tti_ms: 3800

# Workflow context directory
context_dir: .claude/context/browser-testing/{{workflow_id}}

steps:
  # Phase 0: Pre-validation
  - step: 0.5
    name: 'DevTools Availability Check'
    agent: orchestrator
    inputs: []
    outputs:
      - devtools-availability-{{workflow_id}}.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/00.5-orchestrator.json
    validation:
      check: 'chrome-devtools-mcp-available'
      gate: .claude/context/history/gates/{{workflow_id}}/00.5-orchestrator.json
    description: |
      Verify Chrome DevTools MCP is available before browser testing:
      - Check if Chrome DevTools MCP server is configured in `.claude/.mcp.json`
      - Verify required tools are available: navigate_page, take_screenshot, click_element, type_text, get_console_logs, get_network_logs, performance_profiling
      - If DevTools unavailable: Fail gracefully with clear error message
      - If available: Proceed to planning phase
      - Create devtools-availability-{{workflow_id}}.json with availability status

  # Phase 0: Planning
  - step: 0
    name: 'Planning Phase'
    agent: planner
    inputs:
      - target_url
      - documentation_sources
      - test_scope
      - performance_thresholds
    outputs:
      - plan-{{workflow_id}}.md
      - plan-{{workflow_id}}.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/00-planner.json
    validation:
      schema: .claude/schemas/plan.schema.json
      gate: .claude/context/history/gates/{{workflow_id}}/00-planner.json
    description: |
      Plan browser testing workflow:
      - Analyze testing requirements and scope
      - **Check available Chrome DevTools MCP tools**: Review `.claude/.mcp.json` to understand available browser automation capabilities (navigate_page, take_screenshot, click_element, type_text, get_console_logs, get_network_logs, performance_profiling)
      - Identify documentation sources for comparison
      - Define testing strategy and priorities
      - Plan feature discovery approach (DOM traversal, accessibility tree, sitemap analysis)
      - Plan documentation parsing strategy (PRD, UX spec, user stories)
      - Plan per-feature performance measurement approach
      - Plan log-feature correlation strategy
      - Create structured testing plan

  - step: 0.1
    name: 'Plan Rating Gate'
    agent: orchestrator
    type: validation
    skill: response-rater
    inputs:
      - plan-{{workflow_id}}.json (from step 0)
    outputs:
      - .claude/context/runs/{{run_id}}/plans/{{plan_id}}-rating.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/00.1-orchestrator.json
    validation:
      minimum_score: 7
      rubric_file: .claude/context/artifacts/standard-plan-rubric.json
      gate: .claude/context/history/gates/{{workflow_id}}/00.1-orchestrator.json
    retry:
      max_attempts: 3
      on_failure: escalate_to_human
    description: |
      Rate plan quality using response-rater skill.
      - Rubric: completeness, feasibility, risk mitigation, agent coverage, integration
      - Minimum passing score: 7/10
      - If score < 7: Return to Planner with feedback, request improvements, re-rate
      - If score >= 7: Proceed with workflow execution

  # Phase 1: Browser Initialization & Page Load
  - step: 1
    name: 'Browser Initialization & Page Load'
    agent: developer
    inputs:
      - plan-{{workflow_id}}.json (from step 0)
      - target_url
    outputs:
      - browser-session-{{workflow_id}}.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/01-developer.json
    validation:
      schema: .claude/schemas/browser-session.schema.json
      gate: .claude/context/history/gates/{{workflow_id}}/01-developer.json
    description: |
      Initialize browser session and capture initial page load:
      - **CRITICAL: Enable DevTools features BEFORE navigation**:
        - Enable network monitoring to capture all network requests
        - Enable console logging to capture all console messages
        - Enable performance profiling to measure Core Web Vitals
        - Enable memory profiling to track memory usage
        - Enable accessibility inspection for feature discovery
      - Navigate to target URL using Chrome DevTools MCP `navigate_page` tool
      - Capture initial page load metrics (load time, TTFB, resources)
      - Take initial screenshot
      - Record browser session metadata (browser info, viewport, DevTools features enabled)
      - Handle errors gracefully: If Chrome DevTools MCP fails or browser crashes, document the error and attempt session recovery
      - Create browser-session-{{workflow_id}}.json

  # Phase 2: Comprehensive UI Feature Testing
  - step: 2
    name: 'Comprehensive UI Feature Testing'
    agent: qa
    inputs:
      - plan-{{workflow_id}}.json (from step 0)
      - browser-session-{{workflow_id}}.json (from step 1)
      - documentation_sources
      - target_url
    outputs:
      - ui-test-results-{{workflow_id}}.json
      - documentation-comparison-{{workflow_id}}.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/02-qa.json
    validation:
      schema: .claude/schemas/ui-test-results.schema.json
      gate: .claude/context/history/gates/{{workflow_id}}/02-qa.json
    description: |
      Systematic UI feature testing:
      - **Feature Discovery Strategy** (discover ALL UI features before testing):
        - DOM traversal: Use querySelectorAll to find all interactive elements (buttons, links, forms, inputs)
        - Accessibility tree: Analyze ARIA labels and roles to discover hidden features
        - Sitemap/route analysis: For SPAs, analyze routing configuration to discover all routes/pages
        - Authentication flow discovery: Test login/logout flows to access protected features
        - Hidden feature discovery: Trigger modals, dropdowns, tooltips, context menus, hover states
        - Dynamic content patterns: Test lazy-loaded content, infinite scroll, dynamic route changes, client-side routing
        - Network analysis: Review network requests to identify API endpoints and data-driven features
      - **Documentation Parsing** (parse different documentation formats):
        - PRD parsing: Extract features from markdown sections, structured feature lists, acceptance criteria
        - UX spec parsing: Extract user flows, interaction patterns, design specifications from design files
        - User story parsing: Parse Given-When-Then format, extract feature descriptions and acceptance criteria
        - Feature extraction: Build structured feature list from documentation with expected behaviors
        - Comparison algorithm: For each documented feature, compare expected vs actual behavior, calculate accuracy score
      - Test ALL discovered UI features systematically (navigation, forms, interactive elements, data display)
      - For each feature: document expected behavior (from documentation), test actual behavior, capture screenshots with timestamps
      - Record interaction timing for each feature test (for per-feature performance measurement in Step 4)
      - Tag each feature test with unique identifier and timestamp (for log correlation in Step 3)
      - Capture console errors/warnings during feature testing with timestamps
      - Compare actual behavior with documentation (PRD, UX spec, user stories)
      - Identify discrepancies (missing features, undocumented features, behavior mismatches)
      - Create ui-test-results-{{workflow_id}}.json and documentation-comparison-{{workflow_id}}.json

  # Phase 3: Log Review & Error Analysis
  - step: 3
    name: 'Log Review & Error Analysis'
    agent: developer
    inputs:
      - plan-{{workflow_id}}.json (from step 0)
      - browser-session-{{workflow_id}}.json (from step 1)
      - ui-test-results-{{workflow_id}}.json (from step 2)
      - target_url
    outputs:
      - log-analysis-{{workflow_id}}.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/03-developer.json
    validation:
      schema: .claude/schemas/log-analysis.schema.json
      gate: .claude/context/history/gates/{{workflow_id}}/03-developer.json
    description: |
      Analyze browser logs for errors and issues:
      - Extract console logs (errors, warnings, info) using Chrome DevTools MCP `get_console_logs` tool
      - Capture network logs (failed requests, slow requests, CORS errors) using `get_network_logs` tool
      - Review performance logs (long tasks, layout shifts, memory usage)
      - **Log-Feature Correlation** (CRITICAL):
        - Cross-reference log timestamps with feature test timestamps from Step 2 (ui-test-results-{{workflow_id}}.json)
        - Tag each log entry with the feature identifier that was being tested when the log occurred
        - Correlate console errors with specific UI features tested
        - Correlate network failures with specific feature interactions
        - Create feature-log mapping to identify which features trigger which errors
      - Categorize by severity and impact
      - Create log-analysis-{{workflow_id}}.json with feature correlations

  # Phase 4: Performance Timing & Measurement
  - step: 4
    name: 'Performance Timing & Measurement'
    agent: performance-engineer
    inputs:
      - plan-{{workflow_id}}.json (from step 0)
      - browser-session-{{workflow_id}}.json (from step 1)
      - ui-test-results-{{workflow_id}}.json (from step 2)
      - performance_thresholds
      - target_url
    outputs:
      - performance-metrics-{{workflow_id}}.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/04-performance-engineer.json
    validation:
      schema: .claude/schemas/performance-metrics.schema.json
      gate: .claude/context/history/gates/{{workflow_id}}/04-performance-engineer.json
    description: |
      Measure browser performance metrics:
      - **Initial Page Load Performance**:
        - Measure Core Web Vitals (LCP, FID, CLS, FCP, TTI) using Chrome DevTools MCP `performance_profiling` tool
        - Time API response times (average, P95, P99) for initial page load
        - Analyze resource sizes (bundles, images) for initial load
      - **Per-Feature Performance Measurement** (CRITICAL - measure ALL UI features):
        - For each major UI feature tested in Step 2, measure performance during interaction:
          - Use `performance_profiling` tool during each feature interaction
          - Measure interaction response time (time from user action to UI update)
          - Measure render time for feature-specific UI updates
          - Measure API response times for feature-specific network requests
          - Measure layout shifts (CLS) caused by feature interactions
          - Measure input delay (FID) for interactive features
        - Cross-reference with ui-test-results-{{workflow_id}}.json to get feature list and timestamps
        - Create per-feature performance metrics (not just initial page load)
      - Identify slowest API endpoints (both initial load and per-feature)
      - Measure component render times (initial and per-feature)
      - Analyze resource sizes (bundles, images)
      - Identify unused CSS/JS
      - Evaluate cache efficiency
      - Identify performance bottlenecks (initial load and per-feature)
      - Create performance-metrics-{{workflow_id}}.json with both initial load and per-feature metrics

  # Phase 5: UI Optimization Analysis
  - step: 5
    name: 'UI Optimization Analysis'
    agent: ux-expert
    parallel: true
    inputs:
      - plan-{{workflow_id}}.json (from step 0)
      - ui-test-results-{{workflow_id}}.json (from step 2)
      - performance-metrics-{{workflow_id}}.json (from step 4)
      - documentation-comparison-{{workflow_id}}.json (from step 2)
    outputs:
      - ui-optimization-analysis-{{workflow_id}}.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/05-ux-expert.json
    validation:
      schema: .claude/schemas/ui-optimization-analysis.schema.json
      gate: .claude/context/history/gates/{{workflow_id}}/05-ux-expert.json
    description: |
      Analyze UI/UX optimization opportunities:
      - Review UI test results for UX improvements
      - Analyze performance metrics for UI-related bottlenecks
      - Identify accessibility improvements
      - Recommend visual design enhancements
      - Suggest interaction improvements
      - Create prioritized optimization recommendations with categories, priorities, estimated effort, and impact
      - Follow ui-optimization-analysis.schema.json structure

  - step: 5.3
    name: 'Accessibility Testing'
    agent: accessibility-expert
    parallel: true
    inputs:
      - plan-{{workflow_id}}.json (from step 0)
      - ui-test-results-{{workflow_id}}.json (from step 2)
      - browser-session-{{workflow_id}}.json (from step 1)
    outputs:
      - accessibility-audit-{{workflow_id}}.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/05.3-accessibility-expert.json
    validation:
      gate: .claude/context/history/gates/{{workflow_id}}/05.3-accessibility-expert.json
    description: |
      Web accessibility testing and audit:
      - WCAG 2.1 AA/AAA compliance testing
      - Screen reader compatibility (NVDA, JAWS, VoiceOver)
      - Keyboard navigation testing
      - Color contrast analysis
      - ARIA attribute validation
      - Focus management review
      - Alternative text verification
      - Form accessibility audit
      - Semantic HTML validation
      - Automated accessibility testing (axe, Lighthouse)

  - step: 5.5
    name: 'Performance Optimization Analysis'
    agent: performance-engineer
    parallel: true
    inputs:
      - plan-{{workflow_id}}.json (from step 0)
      - performance-metrics-{{workflow_id}}.json (from step 4)
      - log-analysis-{{workflow_id}}.json (from step 3)
    outputs:
      - performance-optimization-analysis-{{workflow_id}}.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/05.5-performance-engineer.json
    validation:
      schema: .claude/schemas/performance-optimization-analysis.schema.json
      gate: .claude/context/history/gates/{{workflow_id}}/05.5-performance-engineer.json
    description: |
      Analyze performance optimization opportunities:
      - Review performance metrics and bottlenecks (both initial load and per-feature)
      - Identify code splitting opportunities
      - Recommend image optimization strategies
      - Suggest caching improvements
      - Recommend API optimization strategies
      - Create prioritized performance optimization recommendations with categories, priorities, estimated improvements, and effort
      - Follow performance-optimization-analysis.schema.json structure

  # Phase 5.7: Code Review of Findings
  - step: 5.7
    name: 'Code Review of Findings'
    agent: code-reviewer
    inputs:
      - plan-{{workflow_id}}.json (from step 0)
      - log-analysis-{{workflow_id}}.json (from step 3)
      - ui-optimization-analysis-{{workflow_id}}.json (from step 5)
      - performance-optimization-analysis-{{workflow_id}}.json (from step 5.5)
      - accessibility-audit-{{workflow_id}}.json (from step 5.3)
    outputs:
      - findings-code-review-{{workflow_id}}.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/05.7-code-reviewer.json
    validation:
      gate: .claude/context/history/gates/{{workflow_id}}/05.7-code-reviewer.json
    description: |
      Code review of findings and recommendations:
      - Review identified bugs and errors from logs
      - Validate performance optimization recommendations
      - Assess code quality issues flagged by testing
      - Verify accessibility issues at code level
      - Prioritize fixes based on severity and impact
      - Identify quick wins vs complex refactoring
      - Create actionable code improvement plan

  # Phase 6: Comprehensive Report & Recommendations
  - step: 6
    name: 'Comprehensive Report & Recommendations'
    agent: developer
    inputs:
      - plan-{{workflow_id}}.json (from step 0)
      - browser-session-{{workflow_id}}.json (from step 1)
      - ui-test-results-{{workflow_id}}.json (from step 2)
      - log-analysis-{{workflow_id}}.json (from step 3)
      - performance-metrics-{{workflow_id}}.json (from step 4)
      - documentation-comparison-{{workflow_id}}.json (from step 2)
      - ui-optimization-analysis-{{workflow_id}}.json (from step 5)
      - performance-optimization-analysis-{{workflow_id}}.json (from step 5.5)
    outputs:
      - browser-test-report-{{workflow_id}}.json
      - browser-test-report-{{workflow_id}}.md
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/06-developer.json
    validation:
      schema: .claude/schemas/browser-test-report.schema.json
      gate: .claude/context/history/gates/{{workflow_id}}/06-developer.json
    description: |
      Generate comprehensive browser testing report:
      - Consolidate all findings from UI testing, log analysis, performance measurement
      - Identify and document all bugs found (prioritized by severity)
      - Create optimization opportunities (UI/UX and performance)
      - Generate prioritized recommendations with effort estimates
      - Create implementation phases
      - Generate browser-test-report-{{workflow_id}}.json and browser-test-report-{{workflow_id}}.md

  # Phase 7: Quality Validation
  - step: 7
    name: 'Quality Validation'
    agent: qa
    inputs:
      - plan-{{workflow_id}}.json (from step 0)
      - browser-test-report-{{workflow_id}}.json (from step 6)
      - ui-test-results-{{workflow_id}}.json (from step 2)
    outputs:
      - quality-validation-{{workflow_id}}.json
      - reasoning: .claude/context/history/reasoning/{{workflow_id}}/07-qa.json
    validation:
      schema: .claude/schemas/quality-validation.schema.json
      gate: .claude/context/history/gates/{{workflow_id}}/07-qa.json
    description: |
      Validate testing quality and report completeness:
      - Verify all critical features were tested (calculate features_tested_percentage)
      - Validate bug reports are complete and actionable
      - Check that all recommendations are prioritized correctly
      - Ensure documentation comparison is accurate
      - Validate performance metrics are complete (both initial load and per-feature)
      - Check completeness: all critical features tested, bug reports complete, recommendations prioritized, documentation accurate, performance metrics complete
      - Create quality validation report following quality-validation.schema.json structure

  - step: 7.5
    name: 'Publish Artifacts'
    agent: orchestrator
    skill: artifact-publisher
    condition: "validation_status == 'pass'"
    inputs:
      - artifact-registry.json
    policy: auto-on-pass
    retry:
      max_attempts: 3
      on_failure: log_and_continue
