"""
RAG Evaluation Functions

Evaluation utilities for RAG systems including retrieval and end-to-end evaluation.
Based on Claude Cookbooks patterns.
"""

import os
from typing import Any

from anthropic import Anthropic

from .metrics import evaluate_retrieval


def evaluate_retrieval_quality(
    retrieved_chunks: list[str],
    correct_chunks: list[str]
) -> dict[str, Any]:
    """
    Evaluate retrieval quality using precision, recall, F1, and MRR.
    
    Args:
        retrieved_chunks: List of retrieved chunk identifiers
        correct_chunks: List of correct/relevant chunk identifiers
        
    Returns:
        Dictionary with metrics and evaluation result
    """
    metrics = evaluate_retrieval(retrieved_chunks, correct_chunks)
    
    return {
        "metrics": metrics,
        "pass": metrics["f1"] >= 0.3,  # Threshold for passing
        "reason": (
            f"Precision: {metrics['precision']:.3f}, "
            f"Recall: {metrics['recall']:.3f}, "
            f"F1: {metrics['f1']:.3f}, "
            f"MRR: {metrics['mrr']:.3f}"
        )
    }


def evaluate_answer_quality(
    query: str,
    generated_answer: str,
    correct_answer: str,
    model: str = "claude-sonnet-4-5",
    api_key: str | None = None
) -> dict[str, Any]:
    """
    Evaluate answer quality using LLM-as-judge pattern.
    
    Args:
        query: The original question/query
        generated_answer: Answer generated by the RAG system
        correct_answer: Ground truth correct answer
        model: Claude model to use for evaluation
        api_key: Anthropic API key
        
    Returns:
        Dictionary with evaluation results
    """
    from ..evaluation.end_to_end_eval import evaluate_answer_quality as eval_func
    
    return eval_func(
        query=query,
        generated_answer=generated_answer,
        correct_answer=correct_answer,
        model=model,
        api_key=api_key
    )


def evaluate_rag_system(
    test_cases: list[dict[str, Any]],
    retrieval_func: callable,
    answer_func: callable,
    model: str = "claude-sonnet-4-5"
) -> dict[str, Any]:
    """
    Evaluate complete RAG system with both retrieval and end-to-end metrics.
    
    Args:
        test_cases: List of test cases with query, correct_chunks, correct_answer
        retrieval_func: Function that retrieves chunks for a query
        answer_func: Function that generates answers from chunks
        model: Claude model for evaluation
        
    Returns:
        Dictionary with overall metrics and per-case results
    """
    retrieval_results = []
    end_to_end_results = []
    
    total_precision = 0.0
    total_recall = 0.0
    total_f1 = 0.0
    total_mrr = 0.0
    total_accuracy = 0.0
    
    for case in test_cases:
        query = case.get("query", "")
        correct_chunks = case.get("correct_chunks", [])
        correct_answer = case.get("correct_answer", "")
        
        # Evaluate retrieval
        retrieved_chunks = retrieval_func(query)
        retrieval_metrics = evaluate_retrieval(retrieved_chunks, correct_chunks)
        
        retrieval_results.append({
            "query": query,
            "retrieved": retrieved_chunks,
            "correct": correct_chunks,
            "metrics": retrieval_metrics
        })
        
        total_precision += retrieval_metrics["precision"]
        total_recall += retrieval_metrics["recall"]
        total_f1 += retrieval_metrics["f1"]
        total_mrr += retrieval_metrics["mrr"]
        
        # Evaluate end-to-end
        generated_answer = answer_func(query, retrieved_chunks)
        end_to_end_eval = evaluate_answer_quality(
            query=query,
            generated_answer=generated_answer,
            correct_answer=correct_answer,
            model=model
        )
        
        end_to_end_results.append({
            "query": query,
            "generated_answer": generated_answer,
            "correct_answer": correct_answer,
            "evaluation": end_to_end_eval
        })
        
        if end_to_end_eval["is_correct"]:
            total_accuracy += 1.0
    
    n = len(test_cases) if test_cases else 1
    
    return {
        "retrieval_metrics": {
            "average_precision": total_precision / n,
            "average_recall": total_recall / n,
            "average_f1": total_f1 / n,
            "average_mrr": total_mrr / n,
        },
        "end_to_end_metrics": {
            "accuracy": total_accuracy / n,
        },
        "retrieval_results": retrieval_results,
        "end_to_end_results": end_to_end_results,
        "total_tests": n,
    }

