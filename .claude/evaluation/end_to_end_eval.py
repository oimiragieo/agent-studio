"""
End-to-end evaluation for RAG systems using LLM-as-judge.

Based on Claude Cookbooks patterns, this module evaluates the quality of
generated answers by comparing them to correct answers using Claude.
"""

import os
import re
from typing import Any

from anthropic import Anthropic


def evaluate_end_to_end(
    query: str, generated_answer: str, correct_answer: str
) -> dict[str, Any]:
    """
    Evaluate end-to-end RAG performance using LLM-as-judge.
    
    Uses Claude to determine if a generated answer is correct based on
    substance and meaning, not exact wording.
    
    Args:
        query: The original question
        generated_answer: Answer generated by the RAG system
        correct_answer: The correct/reference answer
        
    Returns:
        Dictionary with evaluation results:
        - question: Original query
        - correct_answer: Reference answer
        - generated_answer: System-generated answer
        - is_correct: Boolean indicating correctness
        - explanation: LLM's explanation of the evaluation
    """
    prompt = f"""
You are an AI assistant tasked with evaluating the correctness of answers to questions.

Question: {query}

Correct Answer: {correct_answer}

Generated Answer: {generated_answer}

Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ.

Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct.

However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect.

Finally, if there are any direct contradictions between the correct answer and generated answer, we should deem the generated answer to be incorrect.

Respond in the following XML format:
<evaluation>
<content>
<explanation>Your explanation here</explanation>
<is_correct>true/false</is_correct>
</content>
</evaluation>
"""

    client = Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
    
    try:
        response = client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=1500,
            messages=[
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": "<evaluation>"},
            ],
            temperature=0,
            stop_sequences=["</evaluation>"],
        )

        response_text = response.content[0].text

        # Extract explanation and is_correct from XML
        explanation_match = re.search(
            r"<explanation>(.*?)</explanation>", response_text, re.DOTALL
        )
        is_correct_match = re.search(
            r"<is_correct>(.*?)</is_correct>", response_text, re.DOTALL
        )

        if explanation_match and is_correct_match:
            explanation = explanation_match.group(1).strip()
            is_correct = is_correct_match.group(1).strip().lower() == "true"
        else:
            raise ValueError(
                "Could not extract explanation or is_correct from response"
            )

        result = {
            "question": query,
            "correct_answer": correct_answer,
            "generated_answer": generated_answer,
            "is_correct": is_correct,
            "explanation": explanation,
        }

    except Exception as e:
        result = {
            "question": query,
            "correct_answer": correct_answer,
            "generated_answer": generated_answer,
            "is_correct": False,
            "explanation": f"Unexpected error: {str(e)}",
        }

    return result


def get_assert(output: str, context: dict[str, Any]) -> dict[str, Any]:
    """
    Promptfoo assertion function for end-to-end evaluation.
    
    This function is called by Promptfoo to evaluate generated answers.
    It uses LLM-as-judge to determine correctness.
    
    Args:
        output: Generated answer from the RAG system
        context: Promptfoo context containing test variables
        
    Returns:
        Dictionary with evaluation results:
        - pass: Boolean indicating if answer is correct
        - score: 1.0 if correct, 0.0 if incorrect
        - reason: LLM's explanation of the evaluation
    """
    vars_dict = context.get("vars", {})
    correct_answer = vars_dict.get("correct_answer", "")
    query = vars_dict.get("query", "")
    
    result = evaluate_end_to_end(query, output, correct_answer)
    score = 1.0 if result["is_correct"] else 0.0

    return {
        "pass": result["is_correct"],
        "score": score,
        "reason": result["explanation"],
    }
