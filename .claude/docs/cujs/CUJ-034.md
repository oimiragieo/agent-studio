# CUJ-034: Browser-Based UI Testing & Optimization

## User Goal

Find bugs, test all UI features, compare with documentation, review logs, measure performance, and get optimization recommendations for a web application.

**Persona**: Developer

**Execution Mode**: `browser-testing-flow.yaml`

## Trigger

- "Find bugs using Chrome DevTools"
- "Test all UI features"
- "Test page and find issues"
- "Browser test the application"
- "Test UI and optimize"
- "Find bugs browser"
- "Test all features and compare with documentation"
- "Review logs and optimize performance"

## Workflow

### Step 0: Planning Phase
**Agent**: `planner`
**Inputs**: `target_url`, `documentation_sources`, `test_scope`, `performance_thresholds`
**Outputs**: `plan-{{workflow_id}}.json`
**Description**: 
- Plan browser testing workflow, analyze requirements, define testing strategy
- Check available Chrome DevTools MCP tools in `.claude/.mcp.json` to understand browser automation capabilities
- Plan feature discovery approach (DOM traversal, accessibility tree, sitemap analysis)
- Plan documentation parsing strategy (PRD, UX spec, user stories)
- Plan per-feature performance measurement approach
- Plan log-feature correlation strategy

### Step 1: Browser Initialization & Page Load
**Agent**: `developer`
**Inputs**: `plan-{{workflow_id}}.json`, `target_url`
**Outputs**: `browser-session-{{workflow_id}}.json`
**Tools**: Chrome DevTools MCP (`navigate_page`, `take_screenshot`)
**Description**: 
- **CRITICAL: Enable DevTools features BEFORE navigation**:
  - Enable network monitoring to capture all network requests
  - Enable console logging to capture all console messages
  - Enable performance profiling to measure Core Web Vitals
  - Enable memory profiling to track memory usage
  - Enable accessibility inspection for feature discovery
- Navigate to target URL using Chrome DevTools MCP `navigate_page` tool
- Capture initial page load metrics (load time, TTFB, resources)
- Take initial screenshot
- Record browser session metadata (browser info, viewport, DevTools features enabled)
- Handle errors gracefully: If Chrome DevTools MCP fails or browser crashes, document the error and attempt session recovery

### Step 2: Comprehensive UI Feature Testing
**Agent**: `qa`
**Inputs**: `plan-{{workflow_id}}.json`, `browser-session-{{workflow_id}}.json`, `documentation_sources`, `target_url`
**Outputs**: `ui-test-results-{{workflow_id}}.json`, `documentation-comparison-{{workflow_id}}.json`
**Tools**: Chrome DevTools MCP (`take_screenshot`, `navigate_page`, `click_element`, `type_text`, `get_console_logs`)
**Description**:
- **Feature Discovery Strategy** (discover ALL UI features before testing):
  - DOM traversal: Use querySelectorAll to find all interactive elements (buttons, links, forms, inputs)
  - Accessibility tree: Analyze ARIA labels and roles to discover hidden features
  - Sitemap/route analysis: For SPAs, analyze routing configuration to discover all routes/pages
  - Authentication flow discovery: Test login/logout flows to access protected features
  - Hidden feature discovery: Trigger modals, dropdowns, tooltips, context menus, hover states
  - Dynamic content patterns: Test lazy-loaded content, infinite scroll, dynamic route changes, client-side routing
  - Network analysis: Review network requests to identify API endpoints and data-driven features
- **Documentation Parsing** (parse different documentation formats):
  - PRD parsing: Extract features from markdown sections, structured feature lists, acceptance criteria
  - UX spec parsing: Extract user flows, interaction patterns, design specifications from design files
  - User story parsing: Parse Given-When-Then format, extract feature descriptions and acceptance criteria
  - Feature extraction: Build structured feature list from documentation with expected behaviors
  - Comparison algorithm: For each documented feature, compare expected vs actual behavior, calculate accuracy score
- Test ALL discovered UI features systematically (navigation, forms, interactive elements, data display)
- For each feature: document expected behavior (from documentation), test actual behavior, capture screenshots with timestamps
- Record interaction timing for each feature test (for per-feature performance measurement in Step 4)
- Tag each feature test with unique identifier and timestamp (for log correlation in Step 3)
- Capture console errors/warnings during feature testing with timestamps
- Compare actual behavior with documentation (PRD, UX spec, user stories)
- Identify discrepancies (missing features, undocumented features, behavior mismatches)

### Step 3: Log Review & Error Analysis
**Agent**: `developer`
**Inputs**: `plan-{{workflow_id}}.json`, `browser-session-{{workflow_id}}.json`, `ui-test-results-{{workflow_id}}.json`, `target_url`
**Outputs**: `log-analysis-{{workflow_id}}.json`
**Tools**: Chrome DevTools MCP (`get_console_logs`, `get_network_logs`)
**Description**:
- Extract console logs (errors, warnings, info) using Chrome DevTools MCP `get_console_logs` tool
- Capture network logs (failed requests, slow requests, CORS errors) using `get_network_logs` tool
- Review performance logs (long tasks, layout shifts, memory usage)
- **Log-Feature Correlation** (CRITICAL):
  - Cross-reference log timestamps with feature test timestamps from Step 2 (ui-test-results-{{workflow_id}}.json)
  - Tag each log entry with the feature identifier that was being tested when the log occurred
  - Correlate console errors with specific UI features tested
  - Correlate network failures with specific feature interactions
  - Create feature-log mapping to identify which features trigger which errors
- Categorize by severity and impact

### Step 4: Performance Timing & Measurement
**Agent**: `performance-engineer`
**Inputs**: `plan-{{workflow_id}}.json`, `browser-session-{{workflow_id}}.json`, `ui-test-results-{{workflow_id}}.json`, `performance_thresholds`, `target_url`
**Outputs**: `performance-metrics-{{workflow_id}}.json`
**Tools**: Chrome DevTools MCP (`performance_profiling`, `get_network_logs`)
**Description**:
- **Initial Page Load Performance**:
  - Measure Core Web Vitals (LCP, FID, CLS, FCP, TTI) using Chrome DevTools MCP `performance_profiling` tool
  - Time API response times (average, P95, P99) for initial page load
  - Analyze resource sizes (bundles, images) for initial load
- **Per-Feature Performance Measurement** (CRITICAL - measure ALL UI features):
  - For each major UI feature tested in Step 2, measure performance during interaction:
    - Use `performance_profiling` tool during each feature interaction
    - Measure interaction response time (time from user action to UI update)
    - Measure render time for feature-specific UI updates
    - Measure API response times for feature-specific network requests
    - Measure layout shifts (CLS) caused by feature interactions
    - Measure input delay (FID) for interactive features
  - Cross-reference with ui-test-results-{{workflow_id}}.json to get feature list and timestamps
  - Create per-feature performance metrics (not just initial page load)
- Identify slowest API endpoints (both initial load and per-feature)
- Measure component render times (initial and per-feature)
- Analyze resource sizes (bundles, images)
- Identify unused CSS/JS
- Evaluate cache efficiency
- Identify performance bottlenecks (initial load and per-feature)

### Step 5: UI Optimization Analysis
**Agent**: `ux-expert` (parallel)
**Inputs**: `plan-{{workflow_id}}.json`, `ui-test-results-{{workflow_id}}.json`, `performance-metrics-{{workflow_id}}.json`, `documentation-comparison-{{workflow_id}}.json`
**Outputs**: `ui-optimization-analysis-{{workflow_id}}.json`
**Validation**: Schema validation using `ui-optimization-analysis.schema.json`
**Description**: 
- Analyze UI/UX optimization opportunities
- Review UI test results for UX improvements
- Analyze performance metrics for UI-related bottlenecks
- Identify accessibility improvements
- Recommend visual design enhancements
- Suggest interaction improvements
- Create prioritized optimization recommendations with categories, priorities, estimated effort, and impact
- Follow ui-optimization-analysis.schema.json structure

### Step 5.5: Performance Optimization Analysis
**Agent**: `performance-engineer` (parallel)
**Inputs**: `plan-{{workflow_id}}.json`, `performance-metrics-{{workflow_id}}.json`, `log-analysis-{{workflow_id}}.json`
**Outputs**: `performance-optimization-analysis-{{workflow_id}}.json`
**Validation**: Schema validation using `performance-optimization-analysis.schema.json`
**Description**: 
- Analyze performance optimization opportunities
- Review performance metrics and bottlenecks (both initial load and per-feature)
- Identify code splitting opportunities
- Recommend image optimization strategies
- Suggest caching improvements
- Recommend API optimization strategies
- Create prioritized performance optimization recommendations with categories, priorities, estimated improvements, and effort
- Follow performance-optimization-analysis.schema.json structure

### Step 6: Comprehensive Report & Recommendations
**Agent**: `developer`
**Inputs**: All previous step outputs
**Outputs**: `browser-test-report-{{workflow_id}}.json`, `browser-test-report-{{workflow_id}}.md`
**Description**:
- Consolidate all findings
- Identify and document all bugs (prioritized by severity)
- Create optimization opportunities (UI/UX and performance)
- Generate prioritized recommendations with effort estimates
- Create implementation phases

### Step 7: Quality Validation
**Agent**: `qa`
**Inputs**: `plan-{{workflow_id}}.json`, `browser-test-report-{{workflow_id}}.json`, `ui-test-results-{{workflow_id}}.json`
**Outputs**: `quality-validation-{{workflow_id}}.json`
**Validation**: Schema validation using `quality-validation.schema.json`
**Description**: 
- Validate testing quality and report completeness
- Verify all critical features were tested (calculate features_tested_percentage)
- Validate bug reports are complete and actionable
- Check that all recommendations are prioritized correctly
- Ensure documentation comparison is accurate
- Validate performance metrics are complete (both initial load and per-feature)
- Check completeness: all critical features tested, bug reports complete, recommendations prioritized, documentation accurate, performance metrics complete
- Create quality validation report following quality-validation.schema.json structure

## Agents Used

1. **planner**: Planning and strategy
2. **developer**: Browser initialization, log analysis, report generation
3. **qa**: UI feature testing, documentation comparison, quality validation
4. **performance-engineer**: Performance measurement and optimization analysis
5. **ux-expert**: UI/UX optimization analysis

## Skills Used

- **Chrome DevTools MCP**: 
  - `navigate_page`: Navigate to URLs
  - `take_screenshot`: Capture screenshots
  - `click_element`: Click UI elements
  - `type_text`: Type into input fields
  - `get_console_logs`: Extract console logs
  - `get_network_logs`: Capture network requests
  - `performance_profiling`: Measure performance metrics
  - `get_memory_usage`: Analyze memory patterns

## Expected Outputs

1. **browser-session-{{workflow_id}}.json**: Browser session metadata and initial load metrics
2. **ui-test-results-{{workflow_id}}.json**: Systematic UI feature testing results
3. **documentation-comparison-{{workflow_id}}.json**: Comparison of actual vs documented behavior
4. **log-analysis-{{workflow_id}}.json**: Console, network, and performance log analysis
5. **performance-metrics-{{workflow_id}}.json**: Core Web Vitals and response time metrics
6. **ui-optimization-analysis-{{workflow_id}}.json**: UI/UX optimization recommendations
7. **performance-optimization-analysis-{{workflow_id}}.json**: Performance optimization recommendations
8. **browser-test-report-{{workflow_id}}.json**: Comprehensive testing report (JSON)
9. **browser-test-report-{{workflow_id}}.md**: Comprehensive testing report (Markdown)
10. **quality-validation-{{workflow_id}}.json**: Quality validation report

## Success Criteria

- ✅ All UI features systematically discovered and tested (DOM traversal, accessibility tree, sitemap analysis)
- ✅ Feature discovery strategy executed (hidden features, dynamic content, authentication flows)
- ✅ Documentation parsed and compared (PRD, UX spec, user stories with accuracy score)
- ✅ All bugs documented with screenshots and reproduction steps
- ✅ DevTools features enabled before navigation (network monitoring, console logging, performance profiling)
- ✅ Log-feature correlation completed (logs tagged with feature identifiers)
- ✅ Core Web Vitals measured and scored (initial load)
- ✅ Per-feature performance measured (ALL UI features, not just initial load)
- ✅ Performance bottlenecks identified (initial load and per-feature)
- ✅ Optimization opportunities documented (UI/UX and performance) with schemas
- ✅ Prioritized recommendations with effort estimates
- ✅ Implementation phases created
- ✅ Comprehensive report generated
- ✅ Quality validation completed with schema validation

## Example Prompts

### Example 1: Basic Browser Testing
```
Find bugs using Chrome DevTools. Open http://localhost:3000, test all UI features, compare with the PRD, review logs, and give me recommendations to fix the application.
```

### Example 2: Performance-Focused Testing
```
Test the page at http://localhost:3000 using Chrome DevTools. Test all UI features, time the responses, optimize the UI, and come back with recommendations to fix the application. Focus on performance issues.
```

### Example 3: Comprehensive Testing with Documentation
```
Browser test the application at http://localhost:3000. Test all UI features, compare with the UX spec and user stories, review all logs, measure performance, and provide a comprehensive report with optimization recommendations.
```

## Related CUJs

- **CUJ-013**: Code Review (complementary quality assurance)
- **CUJ-015**: Test Generation (test creation vs test execution)
- **CUJ-019**: Performance Optimization (focused performance work)

## Related Workflows

- `browser-testing-flow.yaml`: This CUJ's primary workflow
- `code-quality-flow.yaml`: Code quality improvements
- `ui-perfection-loop.yaml`: UI refinement workflow

## Notes

- Chrome DevTools MCP must be configured in `.claude/.mcp.json`
- Documentation sources (PRD, UX spec, user stories) should be provided as input
- Screenshots are saved to `.claude/context/history/screenshots/{{workflow_id}}/`
- All artifacts follow the workflow ID pattern for traceability
- **Critical**: DevTools features must be enabled BEFORE navigation in Step 1
- **Critical**: Feature discovery must be systematic (DOM traversal, accessibility tree, sitemap analysis)
- **Critical**: Performance measurement includes both initial load AND per-feature metrics
- **Critical**: Log analysis must correlate logs with specific features tested
- All optimization analyses (Steps 5, 5.5) and quality validation (Step 7) use schema validation

