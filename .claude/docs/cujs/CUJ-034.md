# CUJ-034: Browser-Based UI Testing & Optimization

## User Goal

Find bugs, test all UI features, compare with documentation, review logs, measure performance, and get optimization recommendations for a web application.

**Persona**: Developer

## Trigger

- "Find bugs using Chrome DevTools"
- "Test all UI features"
- "Test page and find issues"
- "Browser test the application"
- "Test UI and optimize"
- "Find bugs browser"
- "Test all features and compare with documentation"
- "Review logs and optimize performance"

## Workflow

**Execution Mode**: `browser-testing-flow.yaml`

### Step 0: Planning Phase

**Agent**: `planner`
**Inputs**: `target_url`, `documentation_sources`, `test_scope`, `performance_thresholds`
**Outputs**: `plan-{{workflow_id}}.json`
**Description**:

- Plan browser testing workflow, analyze requirements, define testing strategy
- Check available Chrome DevTools MCP tools in `.claude/.mcp.json` to understand browser automation capabilities
- Plan feature discovery approach (DOM traversal, accessibility tree, sitemap analysis)
- Plan documentation parsing strategy (PRD, UX spec, user stories)
- Plan per-feature performance measurement approach
- Plan log-feature correlation strategy

### Step 0.1: Plan Rating Gate

- Agent: orchestrator
- Type: validation
- Skill: response-rater
- Validates plan quality (minimum score: 7/10)
- Rubric: completeness, feasibility, risk mitigation, agent coverage, integration
- If score < 7: Return to Planner with feedback
- If score >= 7: Proceed to execution
- Records rating in `.claude/context/runs/<run_id>/plans/<plan_id>-rating.json`

### Step 1: Browser Initialization & Page Load

**Agent**: `developer`
**Inputs**: `plan-{{workflow_id}}.json`, `target_url`
**Outputs**: `browser-session-{{workflow_id}}.json`
**Tools**: Chrome DevTools MCP (`navigate_page`, `take_screenshot`)
**Description**:

- **CRITICAL: Enable DevTools features BEFORE navigation**:
  - Enable network monitoring to capture all network requests
  - Enable console logging to capture all console messages
  - Enable performance profiling to measure Core Web Vitals
  - Enable memory profiling to track memory usage
  - Enable accessibility inspection for feature discovery
- Navigate to target URL using Chrome DevTools MCP `navigate_page` tool
- Capture initial page load metrics (load time, TTFB, resources)
- Take initial screenshot
- Record browser session metadata (browser info, viewport, DevTools features enabled)
- Handle errors gracefully: If Chrome DevTools MCP fails or browser crashes, document the error and attempt session recovery

### Step 2: Comprehensive UI Feature Testing

**Agent**: `qa`
**Inputs**: `plan-{{workflow_id}}.json`, `browser-session-{{workflow_id}}.json`, `documentation_sources`, `target_url`
**Outputs**: `ui-test-results-{{workflow_id}}.json`, `documentation-comparison-{{workflow_id}}.json`
**Tools**: Chrome DevTools MCP (`take_screenshot`, `navigate_page`, `click`, `fill`, `list_console_messages`)
**Description**:

- **Feature Discovery Strategy** (discover ALL UI features before testing):
  - DOM traversal: Use querySelectorAll to find all interactive elements (buttons, links, forms, inputs)
  - Accessibility tree: Analyze ARIA labels and roles to discover hidden features
  - Sitemap/route analysis: For SPAs, analyze routing configuration to discover all routes/pages
  - Authentication flow discovery: Test login/logout flows to access protected features
  - Hidden feature discovery: Trigger modals, dropdowns, tooltips, context menus, hover states
  - Dynamic content patterns: Test lazy-loaded content, infinite scroll, dynamic route changes, client-side routing
  - Network analysis: Review network requests to identify API endpoints and data-driven features
- **Documentation Parsing** (parse different documentation formats):
  - PRD parsing: Extract features from markdown sections, structured feature lists, acceptance criteria
  - UX spec parsing: Extract user flows, interaction patterns, design specifications from design files
  - User story parsing: Parse Given-When-Then format, extract feature descriptions and acceptance criteria
  - Feature extraction: Build structured feature list from documentation with expected behaviors
  - Comparison algorithm: For each documented feature, compare expected vs actual behavior, calculate accuracy score
- Test ALL discovered UI features systematically (navigation, forms, interactive elements, data display)
- For each feature: document expected behavior (from documentation), test actual behavior, capture screenshots with timestamps
- Record interaction timing for each feature test (for per-feature performance measurement in Step 4)
- Tag each feature test with unique identifier and timestamp (for log correlation in Step 3)
- Capture console errors/warnings during feature testing with timestamps
- Compare actual behavior with documentation (PRD, UX spec, user stories)
- Identify discrepancies (missing features, undocumented features, behavior mismatches)

### Step 3: Log Review & Error Analysis

**Agent**: `developer`
**Inputs**: `plan-{{workflow_id}}.json`, `browser-session-{{workflow_id}}.json`, `ui-test-results-{{workflow_id}}.json`, `target_url`
**Outputs**: `log-analysis-{{workflow_id}}.json`
**Tools**: Chrome DevTools MCP (`list_console_messages`, `list_network_requests`)
**Description**:

- Extract console logs (errors, warnings, info) using Chrome DevTools MCP `list_console_messages` tool
- Capture network logs (failed requests, slow requests, CORS errors) using `list_network_requests` tool
- Review performance logs (long tasks, layout shifts, memory usage)
- **Log-Feature Correlation** (CRITICAL):
  - Cross-reference log timestamps with feature test timestamps from Step 2 (ui-test-results-{{workflow_id}}.json)
  - Tag each log entry with the feature identifier that was being tested when the log occurred
  - Correlate console errors with specific UI features tested
  - Correlate network failures with specific feature interactions
  - Create feature-log mapping to identify which features trigger which errors
- Categorize by severity and impact

### Step 4: Performance Timing & Measurement

**Agent**: `performance-engineer`
**Inputs**: `plan-{{workflow_id}}.json`, `browser-session-{{workflow_id}}.json`, `ui-test-results-{{workflow_id}}.json`, `performance_thresholds`, `target_url`
**Outputs**: `performance-metrics-{{workflow_id}}.json`
**Tools**: Chrome DevTools MCP (`performance_start_trace`, `performance_stop_trace`, `performance_analyze_insight`, `list_network_requests`)
**Description**:

- **Initial Page Load Performance**:
  - Measure Core Web Vitals (LCP, FID, CLS, FCP, TTI) using Chrome DevTools MCP performance tools
  - Time API response times (average, P95, P99) for initial page load
  - Analyze resource sizes (bundles, images) for initial load
- **Per-Feature Performance Measurement** (CRITICAL - measure ALL UI features):
  - For each major UI feature tested in Step 2, measure performance during interaction:
    - Use performance trace tools (`performance_start_trace`, `performance_stop_trace`, `performance_analyze_insight`) during each feature interaction
    - Measure interaction response time (time from user action to UI update)
    - Measure render time for feature-specific UI updates
    - Measure API response times for feature-specific network requests
    - Measure layout shifts (CLS) caused by feature interactions
    - Measure input delay (FID) for interactive features
  - Cross-reference with ui-test-results-{{workflow_id}}.json to get feature list and timestamps
  - Create per-feature performance metrics (not just initial page load)
- Identify slowest API endpoints (both initial load and per-feature)
- Measure component render times (initial and per-feature)
- Analyze resource sizes (bundles, images)
- Identify unused CSS/JS
- Evaluate cache efficiency
- Identify performance bottlenecks (initial load and per-feature)

### Step 5: UI Optimization Analysis

**Agent**: `ux-expert` (parallel)
**Inputs**: `plan-{{workflow_id}}.json`, `ui-test-results-{{workflow_id}}.json`, `performance-metrics-{{workflow_id}}.json`, `documentation-comparison-{{workflow_id}}.json`
**Outputs**: `ui-optimization-analysis-{{workflow_id}}.json`
**Validation**: Schema validation using `ui-optimization-analysis.schema.json`
**Description**:

- Analyze UI/UX optimization opportunities
- Review UI test results for UX improvements
- Analyze performance metrics for UI-related bottlenecks
- Identify accessibility improvements
- Recommend visual design enhancements
- Suggest interaction improvements
- Create prioritized optimization recommendations with categories, priorities, estimated effort, and impact
- Follow ui-optimization-analysis.schema.json structure

### Step 5.5: Performance Optimization Analysis

**Agent**: `performance-engineer` (parallel)
**Inputs**: `plan-{{workflow_id}}.json`, `performance-metrics-{{workflow_id}}.json`, `log-analysis-{{workflow_id}}.json`
**Outputs**: `performance-optimization-analysis-{{workflow_id}}.json`
**Validation**: Schema validation using `performance-optimization-analysis.schema.json`
**Description**:

- Analyze performance optimization opportunities
- Review performance metrics and bottlenecks (both initial load and per-feature)
- Identify code splitting opportunities
- Recommend image optimization strategies
- Suggest caching improvements
- Recommend API optimization strategies
- Create prioritized performance optimization recommendations with categories, priorities, estimated improvements, and effort
- Follow performance-optimization-analysis.schema.json structure

### Step 6: Comprehensive Report & Recommendations

**Agent**: `developer`
**Inputs**: All previous step outputs
**Outputs**: `browser-test-report-{{workflow_id}}.json`, `browser-test-report-{{workflow_id}}.md`
**Description**:

- Consolidate all findings
- Identify and document all bugs (prioritized by severity)
- Create optimization opportunities (UI/UX and performance)
- Generate prioritized recommendations with effort estimates
- Create implementation phases

### Step 7: Quality Validation

**Agent**: `qa`
**Inputs**: `plan-{{workflow_id}}.json`, `browser-test-report-{{workflow_id}}.json`, `ui-test-results-{{workflow_id}}.json`
**Outputs**: `quality-validation-{{workflow_id}}.json`
**Validation**: Schema validation using `quality-validation.schema.json`
**Description**:

- Validate testing quality and report completeness
- Verify all critical features were tested (calculate features_tested_percentage)
- Validate bug reports are complete and actionable
- Check that all recommendations are prioritized correctly
- Ensure documentation comparison is accurate
- Validate performance metrics are complete (both initial load and per-feature)
- Check completeness: all critical features tested, bug reports complete, recommendations prioritized, documentation accurate, performance metrics complete
- Create quality validation report following quality-validation.schema.json structure

### Step 7.5: Publish Artifacts

- **Agent**: orchestrator
- **Skill**: artifact-publisher
- **Condition**: validation_status == 'pass'
- **Policy**: auto-on-pass
- **Description**: Publish all validated artifacts to project feed for cross-platform visibility
- **Output**: Published artifacts with sharing links

## Agents Used

1. **planner**: Planning and strategy
2. **developer**: Browser initialization, log analysis, report generation
3. **qa**: UI feature testing, documentation comparison, quality validation
4. **performance-engineer**: Performance measurement and optimization analysis
5. **ux-expert**: UI/UX optimization analysis

## Capabilities/Tools Used

- **Chrome DevTools MCP** (Platform-specific tools):
  - `navigate_page`: Navigate to URLs
  - `take_screenshot`: Capture screenshots
  - `click`: Click UI elements
  - `fill`: Type into input fields
  - `list_console_messages`: Extract console logs
  - `list_network_requests`: Capture network requests
  - `performance_start_trace`: Start performance tracing
  - `performance_stop_trace`: Stop performance tracing
  - `performance_analyze_insight`: Analyze performance metrics

## Skills Used

- `plan-generator`: Creates structured plans from requirements
- `response-rater`: Validates plan quality and scores completeness
- `artifact-publisher`: Publishes artifacts to project feed

## Expected Outputs

1. **browser-session-{{workflow_id}}.json**: Browser session metadata and initial load metrics
2. **ui-test-results-{{workflow_id}}.json**: Systematic UI feature testing results
3. **documentation-comparison-{{workflow_id}}.json**: Comparison of actual vs documented behavior
4. **log-analysis-{{workflow_id}}.json**: Console, network, and performance log analysis
5. **performance-metrics-{{workflow_id}}.json**: Core Web Vitals and response time metrics
6. **ui-optimization-analysis-{{workflow_id}}.json**: UI/UX optimization recommendations
7. **performance-optimization-analysis-{{workflow_id}}.json**: Performance optimization recommendations
8. **browser-test-report-{{workflow_id}}.json**: Comprehensive testing report (JSON)
9. **browser-test-report-{{workflow_id}}.md**: Comprehensive testing report (Markdown)
10. **quality-validation-{{workflow_id}}.json**: Quality validation report

## Success Criteria

| Criterion                             | Measurement                     | Target                                                                            |
| ------------------------------------- | ------------------------------- | --------------------------------------------------------------------------------- |
| Plan rating                           | response-rater skill score      | >= 7/10 (recorded in `.claude/context/runs/<run_id>/plans/<plan_id>-rating.json`) |
| Rating recorded                       | Rating file exists in run state | File present at `.claude/context/runs/<run_id>/plans/<plan_id>-rating.json`       |
| All UI features discovered            | Feature discovery report        | Features systematically discovered (DOM, accessibility tree, sitemap)             |
| Feature discovery strategy executed   | Feature discovery completeness  | Hidden features, dynamic content, auth flows tested                               |
| Documentation comparison              | Comparison accuracy score       | PRD, UX spec, user stories compared with accuracy metric                          |
| Bugs documented                       | Bug report completeness         | All bugs with screenshots and reproduction steps                                  |
| DevTools enabled before navigation    | Browser initialization log      | Network, console, performance, memory profiling enabled                           |
| Log-feature correlation               | Correlation mapping             | Logs tagged with feature identifiers                                              |
| Core Web Vitals measured              | Performance metrics             | LCP, FID, CLS, FCP, TTI measured for initial load                                 |
| Per-feature performance measured      | Feature performance metrics     | ALL UI features measured (not just initial load)                                  |
| Performance bottlenecks identified    | Bottleneck analysis             | Initial load and per-feature bottlenecks documented                               |
| Optimization opportunities documented | Schema validation               | UI/UX and performance optimizations with schema compliance                        |
| Prioritized recommendations           | Recommendation report           | Recommendations with effort estimates                                             |
| Implementation phases created         | Phase plan                      | Implementation phases defined                                                     |
| Comprehensive report generated        | Report artifacts                | JSON and Markdown reports complete                                                |
| Quality validation completed          | Schema validation               | Quality validation with schema compliance                                         |

## Example Prompts

### Example 1: Basic Browser Testing

```
Find bugs using Chrome DevTools. Open http://localhost:3000, test all UI features, compare with the PRD, review logs, and give me recommendations to fix the application.
```

### Example 2: Performance-Focused Testing

```
Test the page at http://localhost:3000 using Chrome DevTools. Test all UI features, time the responses, optimize the UI, and come back with recommendations to fix the application. Focus on performance issues.
```

### Example 3: Comprehensive Testing with Documentation

```
Browser test the application at http://localhost:3000. Test all UI features, compare with the UX spec and user stories, review all logs, measure performance, and provide a comprehensive report with optimization recommendations.
```

## Related CUJs

- **CUJ-013**: Code Review (complementary quality assurance)
- **CUJ-015**: Test Generation (test creation vs test execution)
- **CUJ-019**: Performance Optimization (focused performance work)

## Related Workflows

- `browser-testing-flow.yaml`: This CUJ's primary workflow
- `code-quality-flow.yaml`: Code quality improvements
- `ui-perfection-loop.yaml`: UI refinement workflow

## Error Recovery

### Retry Strategy

- **Max Retries**: 3 attempts per step
- **Backoff**: Exponential (1s, 2s, 4s)
- **Retry Triggers**: Transient failures, timeouts, rate limits, browser crashes

### Rollback Procedures

1. **Partial Completion**: Save checkpoint to `.claude/context/runs/{{run_id}}/checkpoint.json`
2. **Failed Validation**: Return to previous passing gate
3. **Critical Failure**: Escalate to human with full context (browser crash recovery)

### Fallback Options

- **Alternative Agent**: If primary agent fails 3x, route to backup agent
  - QA → Developer (testing fallback)
  - Performance-Engineer → Developer (profiling fallback)
  - UX-Expert → QA (analysis fallback)
- **Manual Override**: User can force-proceed with documented risks
- **Graceful Degradation**: Continue with partial feature testing (skip problematic features)

### Recovery Artifacts

- Error log: `.claude/context/runs/{{run_id}}/errors.log`
- Recovery state: `.claude/context/runs/{{run_id}}/recovery-state.json`
- Checkpoint: `.claude/context/runs/{{run_id}}/checkpoint.json`
- Browser session: `.claude/context/runs/{{run_id}}/browser-session.json`

## Notes

- Chrome DevTools MCP must be configured in `.claude/.mcp.json`
- Documentation sources (PRD, UX spec, user stories) should be provided as input
- Screenshots are saved to `.claude/context/history/screenshots/{{workflow_id}}/`
- All artifacts follow the workflow ID pattern for traceability
- **Critical**: DevTools features must be enabled BEFORE navigation in Step 1
- **Critical**: Feature discovery must be systematic (DOM traversal, accessibility tree, sitemap analysis)
- **Critical**: Performance measurement includes both initial load AND per-feature metrics
- **Critical**: Log analysis must correlate logs with specific features tested
- All optimization analyses (Steps 5, 5.5) and quality validation (Step 7) use schema validation
